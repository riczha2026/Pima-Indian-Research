# Diabetes Prediction with Machine Learning  

This repository contains my final project for the **UC Berkeley Extension** Data Science course, where I explored and applied several **machine learning (ML) techniques** to a real-world dataset. The focus of the project was to gain hands-on experience with different ML concepts, including decision trees, regression, clustering, k-Nearest Neighbors (k-NN), Naive Bayes, and neural networks.  

The dataset used is the well-known **[Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)** from Kaggle, which contains health-related information from women of Pima heritage, aged 21 years and older. The goal of the project is to explore the dataset, apply ML models, and evaluate their effectiveness in predicting whether an individual has diabetes.  

---

## üìä Project Objectives  

1. Perform **exploratory data analysis (EDA)** with descriptive statistics and visualizations.  
2. Analyze correlations between health indicators and diabetes outcomes.  
3. Build and evaluate several ML models, including:  
   - Logistic Regression  
   - Decision Trees  
   - k-Nearest Neighbors (k-NN)  
   - Naive Bayes  
   - Neural Networks  
   - Clustering (unsupervised learning)  
4. Compare performance of models using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.  
5. Discuss the strengths and limitations of each algorithm for this dataset.  

---




## üõ†Ô∏è Tools & Libraries  

- Python 3.x  
- NumPy, Pandas  
- Matplotlib, Seaborn  
- Scikit-learn  
- TensorFlow  
- Jupyter Notebook  

---

## üìà Key Insights  

- Certain features such as **glucose concentration, BMI, and insulin levels** were highly correlated with diabetes outcomes.  
- **Logistic regression** and **decision trees** provided interpretable baselines.  
- **Neural networks** captured more complex relationships but required careful tuning.  
- **k-NN** and **Naive Bayes** showed varied performance depending on feature scaling and data preprocessing.  
- **Clustering** revealed some grouping patterns but was less effective for direct prediction.  

---

## üìö Learning Outcomes  

This project gave me practical experience with:  

- Applying multiple supervised and unsupervised ML models.  
- Understanding the trade-off between interpretability and predictive power.  
- Performing feature analysis and data preprocessing.  
- Evaluating models with appropriate performance metrics.  
- Using neural networks as part of an introductory deep learning workflow.  

---

## üìå Acknowledgments  

- **UC Berkeley Extension** Data Science course for providing the foundation and guidance.  
- **Kaggle & UCI Machine Learning Repository** for the Pima Indians Diabetes Dataset.  

---

‚ú® *This project was a valuable step in strengthening my machine learning and data science skills, and it serves as a foundation for more advanced projects in the future.*  
